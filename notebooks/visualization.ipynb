{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lavis.datasets.builders import load_dataset\n",
    "from lavis.models import load_model_and_preprocess\n",
    "from scipy.ndimage import binary_erosion\n",
    "from PIL import Image\n",
    "from typing import List, Tuple\n",
    "from algo import (\n",
    "    PITOME,\n",
    "    TOME,\n",
    "    DIFFRATE,\n",
    "    TOFU,\n",
    "    NONE, \n",
    "    pitome,\n",
    "    tome,\n",
    "    DiffRate,\n",
    "    tofu,\n",
    ")\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from omegaconf import OmegaConf\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from lavis.processors.blip_processors import BlipImageBaseProcessor\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model, vis_processors, text_processors = load_model_and_preprocess(\"blip_image_text_matching\", 'base', device=device,is_eval=True)\n",
    "dataset = load_dataset(\"coco_retrieval\", cfg_path=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlipImageEvalProcessor(BlipImageBaseProcessor):\n",
    "    def __init__(self, image_size=384, mean=None, std=None):\n",
    "        super().__init__(mean=mean, std=std)\n",
    "\n",
    "      \n",
    "        transform_list = [\n",
    "            transforms.Resize(\n",
    "                (image_size, image_size), interpolation=InterpolationMode.BICUBIC\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        # The visualization and model need different transforms\n",
    "        self.transform_vis  = transforms.Compose(transform_list)\n",
    "        self.transform = transforms.Compose(transform_list + [\n",
    "            transforms.ToTensor(),\n",
    "            self.normalize,\n",
    "        ])\n",
    "\n",
    "    def __call__(self, item):\n",
    "        return self.transform_vis(item), self.transform(item)\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, cfg=None):\n",
    "        if cfg is None:\n",
    "            cfg = OmegaConf.create()\n",
    "\n",
    "        image_size = cfg.get(\"image_size\", 384)\n",
    "\n",
    "        mean = cfg.get(\"mean\", None)\n",
    "        std = cfg.get(\"std\", None)\n",
    "\n",
    "        return cls(image_size=image_size, mean=mean, std=std)\n",
    "\n",
    "processor = BlipImageEvalProcessor() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ALGOS = {\n",
    "    PITOME: pitome, \n",
    "    TOME: tome, \n",
    "    DIFFRATE: DiffRate, \n",
    "    TOFU: tofu, \n",
    "}\n",
    "\n",
    "def get_model(model, algo, ratio):\n",
    "    if algo == DIFFRATE:\n",
    "        ALGOS[algo].patch.blip(model.visual_encoder, trace_source=True, output_attn=True)\n",
    "        ALGOS[algo].patch.blip(model.visual_encoder_m, trace_source=True, output_attn=True)\n",
    "        model.visual_encoder.init_kept_num_using_ratio(ratio)\n",
    "        model.visual_encoder_m.init_kept_num_using_ratio(ratio)\n",
    "    elif algo != NONE:\n",
    "        ALGOS[algo].patch.blip(model.visual_encoder, trace_source=True, output_attn=True)\n",
    "        ALGOS[algo].patch.blip(model.visual_encoder_m, trace_source=True, output_attn=True)\n",
    "        model.visual_encoder.ratio=ratio\n",
    "        model.visual_encoder_m.ratio=ratio\n",
    "    else:\n",
    "        ALGOS[TOME].patch.blip(model.visual_encoder, trace_source=True, output_attn=True)\n",
    "        ALGOS[TOME].patch.blip(model.visual_encoder_m, trace_source=True, output_attn=True)\n",
    "        model.visual_encoder.ratio=1.0\n",
    "        model.visual_encoder_m.ratio=1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "\n",
    "def overlay_img(image1, image2, alpha, output_path='overlayed_img.png'):\n",
    "    # Read the images\n",
    "    # print(image1, image2)\n",
    "    image1 = np.array(image1)\n",
    "    image2 = np.array(image2)\n",
    "    image1 = cv2.cvtColor(image1, cv2.COLOR_RGB2BGR)\n",
    "    image2 = cv2.cvtColor(image2, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    overlay = cv2.addWeighted(image2, 1- alpha, image1, alpha, 0)\n",
    "    print(output_path)\n",
    "    cv2.imwrite(output_path, overlay)\n",
    "    return overlay\n",
    "\n",
    "\n",
    "\n",
    "def generate_colormap(N: int, attention_score: torch.Tensor, seed: int = 0) -> List[Tuple[float, float, float]]:\n",
    "  \"\"\"\n",
    "  Generates a colormap with N elements, with a bolder blue base and lightness adjusted based on attention scores.\n",
    "\n",
    "  Args:\n",
    "      N: Number of colors to generate.\n",
    "      attention_score: A torch.Tensor representing the attention scores.\n",
    "          This will be used to modulate the lightness of the blue color.\n",
    "      seed: An optional integer seed for reproducibility.\n",
    "\n",
    "  Returns:\n",
    "      A list of tuples representing RGB color values (0.0 to 1.0).\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "  def adjust_lightness(attention_value):\n",
    "    normalized_attention = (attention_value - attention_score.min()) / (attention_score.max() - attention_score.min())\n",
    "    # lightness_adjustment =  normalized_attention  # Adjust factor for lightness range\n",
    "    base = (0.0, 0.0, 0.0)\n",
    "    adjusted_color = [base[0], base[1]+1.0*normalized_attention, base[2]+1.0*normalized_attention]\n",
    "    return tuple(max(0.0, min(1.0, val)) for val in adjusted_color)\n",
    "\n",
    "  colormap = [adjust_lightness(attention_value) for attention_value in attention_score.flatten().tolist()]\n",
    "  return colormap\n",
    "\n",
    "def make_visualization(\n",
    "    img: Image, source: torch.Tensor, attention_score:torch.Tensor, patch_size: int = 16, class_token: bool = True\n",
    ") -> Image:\n",
    "    \"\"\"\n",
    "    Create a visualization like in the paper.\n",
    "\n",
    "    Args:\n",
    "     -\n",
    "\n",
    "    Returns:\n",
    "     - A PIL image the same size as the input.\n",
    "    \"\"\"\n",
    "\n",
    "    img = np.array(img.convert(\"RGB\")) / 255.0\n",
    "    source = source.detach().cpu()\n",
    "\n",
    "    h, w, _ = img.shape\n",
    "    ph = h // patch_size\n",
    "    pw = w // patch_size\n",
    "\n",
    "    if class_token:\n",
    "        source = source[:, :, 1:]\n",
    "    vis = source.argmax(dim=1)\n",
    "    num_groups = vis.max().item() + 1\n",
    "    print('num_group',num_groups)\n",
    "\n",
    "    cmap = generate_colormap(num_groups, attention_score)\n",
    "    vis_img = 0\n",
    "\n",
    "    for i in range(num_groups):\n",
    "        mask = (vis == i).float().view(1, 1, ph, pw)\n",
    "        mask = F.interpolate(mask, size=(h, w), mode=\"nearest\")\n",
    "        mask = mask.view(h, w, 1).numpy()\n",
    "\n",
    "        color = (mask * img).sum(axis=(0, 1)) / mask.sum()\n",
    "        mask_eroded = binary_erosion(mask[..., 0], iterations=1)[..., None]\n",
    "        mask_edge = mask - mask_eroded\n",
    "\n",
    "        if not np.isfinite(color).all():\n",
    "            color = np.zeros(3)\n",
    "        \n",
    "\n",
    "        vis_img = vis_img + mask_eroded * color.reshape(1, 1, 3) \n",
    "        vis_img = vis_img + mask_edge * 4* np.array(cmap[i]).reshape(1, 1, 3)\n",
    "\n",
    "    # Convert back into a PIL image\n",
    "    vis_img = Image.fromarray(np.uint8(vis_img * 255))\n",
    "\n",
    "    return vis_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def visualize(algo, ratio, index=None, path=None):\n",
    "    path = f'{os.getcwd()}/examples/images/image.jpg'\n",
    "    model, _, _ = load_model_and_preprocess(\"blip_retrieval\", \"coco\", is_eval=False)\n",
    "    get_model(model,algo, ratio)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if index is not None:\n",
    "            img = dataset['train'][index]['image']\n",
    "            print(dataset['train'][index]['text_input'])\n",
    "        else:\n",
    "            img = Image.open(path)\n",
    "\n",
    "        img_vis, img_input = processor(img)\n",
    "        model.visual_encoder(img_input[None,...])\n",
    "        source = model.visual_encoder._info['source']\n",
    "        attn = model.visual_encoder.blocks[-1].attn.attention_map\n",
    "            # print(attn.shape)\n",
    "\n",
    "        # source = model.visual_encoder._info['sources'][1]\n",
    "        # print(source.shape)\n",
    "    merged_img = make_visualization(img_vis, source, attn.sum(1)[:, 0, :], patch_size=16, class_token=True)\n",
    "\n",
    "    # alpha = 0.4\n",
    "    # final_img = overlay_img(img_vis, merged_img, alpha, output_path=f\"{os.getcwd()}/outputs/overlayed_img.jpg\") \n",
    "    # return  Image.fromarray(np.uint8(final_img * 255))\n",
    "    return merged_img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.85\n",
    "# index = 102 \n",
    "# index = 300 \n",
    "# index = 6400 \n",
    "# index = 6665 \n",
    "# index = 21110 \n",
    "# index = 15500 \n",
    "index = 25000 \n",
    "# index = 37000 \n",
    "# index = 47000 \n",
    "# index = 58000 \n",
    "# index = 69000 \n",
    "# index = 25000 \n",
    "visualization = visualize(PITOME, ratio, index)\n",
    "# visualize(PITOME,  ratio)\n",
    "visualization\n",
    "# visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(TOME, ratio, index)\n",
    "# visualize(TOME, ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(TOFU,  ratio, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # visualize(DIFFRATE,  ratio)\n",
    "# visualize(DIFFRATE,  ratio, index)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(NONE, ratio=1.0, index=index)\n",
    "# visualize(NONE, ratio=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pitome",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
