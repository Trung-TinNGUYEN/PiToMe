# import torch
# import torch.nn as nn
# from transformers.models.bart.modeling_bart import BartModel, BartEncoder, BartEncoderLayer, BartAttention, _expand_mask, shift_tokens_right
# from transformers.modeling_outputs import BaseModelOutput, Seq2SeqModelOutput, dataclass, ModelOutput 
# from typing import Optional, Tuple, Union, List
# from ..merge import merge_source, merge_wavg, merge_attention_mask, bipartite_soft_matching


# @dataclass
# class BaseModelOutputWithMask(ModelOutput):
#     last_hidden_state: torch.FloatTensor = None
#     hidden_states: Optional[Tuple[torch.FloatTensor]] = None
#     attentions: Optional[Tuple[torch.FloatTensor]] = None
#     attention_mask: Optional[Tuple[torch.FloatTensor]] = None

# class MCTFBartEncoderLayer(BartEncoderLayer):
#     def init_margin(self, margin):
#         self.margin = margin

#     def compress_x(self, metric, x, attention_mask):
#         ratio = self._tome_info["ratio"].pop(0)
#         if ratio < 1.0:
#             print(ratio)
#             merge, isolated_score = bipartite_soft_matching(
#                 ratio=ratio,
#                 metric=metric,
#                 class_token=self._tome_info["class_token"]
#             )

#             if self._tome_info["trace_source"]:
#                 self._tome_info["source"] = merge_source(
#                     merge, x, self._tome_info["source"]
#                 )
#             if isolated_score is not None and self._tome_info["size"] is not None:
#                 weight = self._tome_info["size"] + isolated_score
#                 x, self._tome_info["size"] = merge_wavg(merge, x, weight)
#             else:
#                 weight = self._tome_info["size"] 
#                 x, self._tome_info["size"] = merge_wavg(merge, x, weight)
#             attention_mask = merge_attention_mask(merge, attention_mask=attention_mask[..., None]).squeeze_(-1)

#         return x, attention_mask


#     def forward(
#         self,
#         hidden_states: torch.FloatTensor,
#         attention_mask: torch.FloatTensor,
#         layer_head_mask: torch.FloatTensor,
#         output_attentions: Optional[bool] = False,
#     ) -> Tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:
#         """
#         Args:
#             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`
#             attention_mask (`torch.FloatTensor`): attention mask of size
#                 `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
#             layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size
#                 `(encoder_attention_heads,)`.
#             output_attentions (`bool`, *optional*):
#                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under
#                 returned tensors for more detail.
#         """
#         residual = hidden_states
#         extended_attention_mask = _expand_mask(attention_mask, hidden_states.dtype)
#         hidden_states, attn_weights, metric ,_ = self.self_attn(
#             hidden_states=hidden_states,
#             attention_mask=extended_attention_mask,
#             layer_head_mask=layer_head_mask,
#             output_attentions=output_attentions,
#         )
#         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)
#         hidden_states = residual + hidden_states
#         hidden_states = self.self_attn_layer_norm(hidden_states)
#         hidden_states, attention_mask = self.compress_x(metric=metric, x=hidden_states, attention_mask=attention_mask)

#         residual = hidden_states
#         hidden_states = self.activation_fn(self.fc1(hidden_states))
#         hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)
#         hidden_states = self.fc2(hidden_states)
#         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)
#         hidden_states = residual + hidden_states
#         hidden_states = self.final_layer_norm(hidden_states)

#         if hidden_states.dtype == torch.float16 and (
#             torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()
#         ):
#             clamp_value = torch.finfo(hidden_states.dtype).max - 1000
#             hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)

#         outputs = (hidden_states, attention_mask,)

#         if output_attentions:
#             outputs += (attn_weights,)

#         return outputs

# class MCTFBartAttention(BartAttention):
#     """Multi-headed attention from 'Attention Is All You Need' paper"""
#     def forward(
#         self,
#         hidden_states: torch.Tensor,
#         key_value_states: Optional[torch.Tensor] = None,
#         past_key_value: Optional[Tuple[torch.Tensor]] = None,
#         attention_mask: Optional[torch.Tensor] = None,
#         layer_head_mask: Optional[torch.Tensor] = None,
#         output_attentions: bool = False,
#     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
#         """Input shape: Batch x Time x Channel"""

#         # if key_value_states are provided this layer is used as a cross-attention layer
#         # for the decoder
#         is_cross_attention = key_value_states is not None

#         bsz, tgt_len, _ = hidden_states.size()

#         # get query proj
#         query_states = self.q_proj(hidden_states) * self.scaling
#         # get key, value proj
#         # `past_key_value[0].shape[2] == key_value_states.shape[1]`
#         # is checking that the `sequence_length` of the `past_key_value` is the same as
#         # the provided `key_value_states` to support prefix tuning
#         if (
#             is_cross_attention
#             and past_key_value is not None
#             and past_key_value[0].shape[2] == key_value_states.shape[1]
#         ):
#             # reuse k,v, cross_attentions
#             key_states = past_key_value[0]
#             value_states = past_key_value[1]
#         elif is_cross_attention:
#             # cross_attentions
#             key_states = self._shape(self.k_proj(key_value_states), -1, bsz)
#             value_states = self._shape(self.v_proj(key_value_states), -1, bsz)
#         elif past_key_value is not None:
#             # reuse k, v, self_attention
#             key_states = self._shape(self.k_proj(hidden_states), -1, bsz)
#             value_states = self._shape(self.v_proj(hidden_states), -1, bsz)
#             key_states = torch.cat([past_key_value[0], key_states], dim=2)
#             value_states = torch.cat([past_key_value[1], value_states], dim=2)
#         else:
#             # self_attention
#             key_states = self._shape(self.k_proj(hidden_states), -1, bsz)
#             value_states = self._shape(self.v_proj(hidden_states), -1, bsz)

#         if self.is_decoder:
#             # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.
#             # Further calls to cross_attention layer can then reuse all cross-attention
#             # key/value_states (first "if" case)
#             # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of
#             # all previous decoder key/value_states. Further calls to uni-directional self-attention
#             # can concat previous decoder key/value_states to current projected key/value_states (third "elif" case)
#             # if encoder bi-directional self-attention `past_key_value` is always `None`
#             past_key_value = (key_states, value_states)

#         proj_shape = (bsz * self.num_heads, -1, self.head_dim)
#         query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)
#         key_states = key_states.reshape(*proj_shape)
#         value_states = value_states.reshape(*proj_shape)

#         src_len = key_states.size(1)
#         attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))

#         if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):
#             raise ValueError(
#                 f"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is"
#                 f" {attn_weights.size()}"
#             )

#         if attention_mask is not None:
#             if attention_mask.size() != (bsz, 1, tgt_len, src_len):
#                 raise ValueError(
#                     f"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}"
#                 )
#             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
#             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)

#         attn_weights = nn.functional.softmax(attn_weights, dim=-1)

#         if layer_head_mask is not None:
#             if layer_head_mask.size() != (self.num_heads,):
#                 raise ValueError(
#                     f"Head mask for a single layer should be of size {(self.num_heads,)}, but is"
#                     f" {layer_head_mask.size()}"
#                 )
#             attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)
#             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)

#         if output_attentions:
#             # this operation is a bit awkward, but it's required to
#             # make sure that attn_weights keeps its gradient.
#             # In order to do so, attn_weights have to be reshaped
#             # twice and have to be reused in the following
#             attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)
#             attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)
#         else:
#             attn_weights_reshaped = None

#         attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)

#         attn_output = torch.bmm(attn_probs, value_states)

#         if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):
#             raise ValueError(
#                 f"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is"
#                 f" {attn_output.size()}"
#             )

#         attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)
#         attn_output = attn_output.transpose(1, 2)

#         # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be
#         # partitioned across GPUs when using tensor-parallelism.
#         attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)

#         attn_output = self.out_proj(attn_output)

#         if len(key_states.shape) == 3:
#             key_states = key_states.mean(0)
#         else:
#             key_states = key_states.mean(1)

#         return attn_output, attn_weights_reshaped, key_states, past_key_value

# class MCTFBartEncoder(BartEncoder):
#     """
#     Modifications:
#     - Initialize r, token size, and token sources.
#     """

#     def forward(
#         self,
#         input_ids: torch.LongTensor = None,
#         attention_mask: Optional[torch.Tensor] = None,
#         head_mask: Optional[torch.Tensor] = None,
#         inputs_embeds: Optional[torch.FloatTensor] = None,
#         output_attentions: Optional[bool] = None,
#         output_hidden_states: Optional[bool] = None,
#         return_dict: Optional[bool] = None,
#     ):
#         r"""
#         Args:
#             input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
#                 Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you
#                 provide it.

#                 Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and
#                 [`PreTrainedTokenizer.__call__`] for details.

#                 [What are input IDs?](../glossary#input-ids)
#             attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
#                 Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:

#                 - 1 for tokens that are **not masked**,
#                 - 0 for tokens that are **masked**.

#                 [What are attention masks?](../glossary#attention-mask)
#             head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):
#                 Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:

#                 - 1 indicates the head is **not masked**,
#                 - 0 indicates the head is **masked**.

#             inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
#                 Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.
#                 This is useful if you want more control over how to convert `input_ids` indices into associated vectors
#                 than the model's internal embedding lookup matrix.
#             output_attentions (`bool`, *optional*):
#                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under
#                 returned tensors for more detail.
#             output_hidden_states (`bool`, *optional*):
#                 Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors
#                 for more detail.
#             return_dict (`bool`, *optional*):
#                 Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
#         """

#         self.total_flop = 0

#         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
#         output_hidden_states = (
#             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
#         )
#         return_dict = return_dict if return_dict is not None else self.config.use_return_dict

#         # retrieve input_ids and inputs_embeds
#         if input_ids is not None and inputs_embeds is not None:
#             raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
#         elif input_ids is not None:
#             input = input_ids
#             input_ids = input_ids.view(-1, input_ids.shape[-1])
#         elif inputs_embeds is not None:
#             input = inputs_embeds[:, :, -1]
#         else:
#             raise ValueError("You have to specify either input_ids or inputs_embeds")

#         if inputs_embeds is None:
#             inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale

#         embed_pos = self.embed_positions(input)
#         embed_pos = embed_pos.to(inputs_embeds.device)

#         hidden_states = inputs_embeds + embed_pos
#         hidden_states = self.layernorm_embedding(hidden_states)
#         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)

#         # expand attention_mask
#         # if attention_mask is not None:
#             # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]
#             # attention_mask = _expand_mask(attention_mask, inputs_embeds.dtype)

#         encoder_states = () if output_hidden_states else None
#         all_attentions = () if output_attentions else None

#         # check if head_mask has a correct number of layers specified if desired
#         if head_mask is not None:
#             if head_mask.size()[0] != (len(self.layers)):
#                 raise ValueError(
#                     f"The head_mask should be specified for {len(self.layers)} layers, but it is for"
#                     f" {head_mask.size()[0]}."
#                 )

#         for idx, encoder_layer in enumerate(self.layers):
#             self.total_flop += self.calculate_block_flop(hidden_states.shape)
#             if output_hidden_states:
#                 encoder_states = encoder_states + (hidden_states,)
#             # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)
#             to_drop = False
#             if self.training:
#                 dropout_probability = torch.rand([])
#                 if dropout_probability < self.layerdrop:  # skip the layer
#                     to_drop = True

#             if to_drop:
#                 layer_outputs = (None, None)
#             else:
#                 if self.gradient_checkpointing and self.training:

#                     def create_custom_forward(module):
#                         def custom_forward(*inputs):
#                             return module(*inputs, output_attentions)

#                         return custom_forward

#                     layer_outputs = torch.utils.checkpoint.checkpoint(
#                         create_custom_forward(encoder_layer),
#                         hidden_states,
#                         attention_mask,
#                         (head_mask[idx] if head_mask is not None else None),
#                     )
#                 else:
#                     layer_outputs = encoder_layer(
#                         hidden_states,
#                         attention_mask,
#                         layer_head_mask=(head_mask[idx] if head_mask is not None else None),
#                         output_attentions=output_attentions,
#                     )

#                 hidden_states = layer_outputs[0]
#                 attention_mask = layer_outputs[1]

#             if output_attentions:
#                 all_attentions = all_attentions + (layer_outputs[1],)

#         if output_hidden_states:
#             encoder_states = encoder_states + (hidden_states,)

#         if not return_dict:
#             return tuple(v for v in [hidden_states, encoder_states, all_attentions, attention_mask] if v is not None)
#         return BaseModelOutputWithMask(
#             last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions, attention_mask=attention_mask
#         )



#     def calculate_block_flop(self, shape):
#         flops = 0
#         _, N, C = shape
#         mhsa_flops = 4*N*C*C + 2*N*N*C
#         flops += mhsa_flops
#         ffn_flops = 8*N*C*C
#         flops += ffn_flops
#         return flops


# def make_tome_class(transformer_class:BartModel):
#     class MCTFBartModel(transformer_class):
#         def init_r(self):
#             len_layers = len(self.encoder.layers)
#             self._tome_info["ratio"] = [self.ratio if i in [
#                 0,1,2
#                 # len_layers - 1, 
#                 # len_layers - 2,
#                 # len_layers - 3,
#                 # len_layers - 9,
#             ] else 1.0 for i in range(len_layers) ]
#             print(self._tome_info['ratio'])
#             self._tome_info["size"] = None
#             self._tome_info["source"] = None
            

#         def forward(
#             self,
#             input_ids: torch.LongTensor = None,
#             attention_mask: Optional[torch.Tensor] = None,
#             decoder_input_ids: Optional[torch.LongTensor] = None,
#             decoder_attention_mask: Optional[torch.LongTensor] = None,
#             head_mask: Optional[torch.Tensor] = None,
#             decoder_head_mask: Optional[torch.Tensor] = None,
#             cross_attn_head_mask: Optional[torch.Tensor] = None,
#             encoder_outputs: Optional[List[torch.FloatTensor]] = None,
#             past_key_values: Optional[List[torch.FloatTensor]] = None,
#             inputs_embeds: Optional[torch.FloatTensor] = None,
#             decoder_inputs_embeds: Optional[torch.FloatTensor] = None,
#             use_cache: Optional[bool] = None,
#             output_attentions: Optional[bool] = None,
#             output_hidden_states: Optional[bool] = None,
#             return_dict: Optional[bool] = None,
#         ) -> Union[Tuple, Seq2SeqModelOutput]:

#             # different to other models, Bart automatically creates decoder_input_ids from
#             # input_ids if no decoder_input_ids are provided
 
#             if decoder_input_ids is None and decoder_inputs_embeds is None:
#                 if input_ids is None:
#                     raise ValueError(
#                         "If no `decoder_input_ids` or `decoder_inputs_embeds` are "
#                         "passed, `input_ids` cannot be `None`. Please pass either "
#                         "`input_ids` or `decoder_input_ids` or `decoder_inputs_embeds`."
#                     )

#                 decoder_input_ids = shift_tokens_right(
#                     input_ids, self.config.pad_token_id, self.config.decoder_start_token_id
#                 )

#             output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
#             output_hidden_states = (
#                 output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
#             )
#             use_cache = use_cache if use_cache is not None else self.config.use_cache
#             return_dict = return_dict if return_dict is not None else self.config.use_return_dict

#             if encoder_outputs is None:
#                 # print(attention_mask.shape)
#                 encoder_outputs = self.encoder(
#                     input_ids=input_ids,
#                     attention_mask=attention_mask,
#                     head_mask=head_mask,
#                     inputs_embeds=inputs_embeds,
#                     output_attentions=output_attentions,
#                     output_hidden_states=output_hidden_states,
#                     return_dict=return_dict,
#                 )
#             # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True
#             elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):
#                 encoder_outputs = BaseModelOutput(
#                     last_hidden_state=encoder_outputs[0],
#                     hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,
#                     attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,
#                 )

#             # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)
#             decoder_outputs = self.decoder(
#                 input_ids=decoder_input_ids,
#                 attention_mask=decoder_attention_mask,
#                 encoder_hidden_states=encoder_outputs[0],
#                 encoder_attention_mask=encoder_outputs[-1].expand(attention_mask.shape[0], encoder_outputs[-1].shape[-1]),
#                 head_mask=decoder_head_mask,
#                 cross_attn_head_mask=cross_attn_head_mask,
#                 past_key_values=past_key_values,
#                 inputs_embeds=decoder_inputs_embeds,
#                 use_cache=use_cache,
#                 output_attentions=output_attentions,
#                 output_hidden_states=output_hidden_states,
#                 return_dict=return_dict,
#             )

#             if not return_dict:
#                 return decoder_outputs + encoder_outputs

#             return Seq2SeqModelOutput(
#                 last_hidden_state=decoder_outputs.last_hidden_state,
#                 past_key_values=decoder_outputs.past_key_values,
#                 decoder_hidden_states=decoder_outputs.hidden_states,
#                 decoder_attentions=decoder_outputs.attentions,
#                 cross_attentions=decoder_outputs.cross_attentions,
#                 encoder_last_hidden_state=encoder_outputs.last_hidden_state,
#                 encoder_hidden_states=encoder_outputs.hidden_states,
#                 encoder_attentions=encoder_outputs.attentions,
#             )

#     return MCTFBartModel


# def apply_patch(
#    model: BartEncoder, trace_source: bool = False, prop_attn: bool = True, margin=0.9, ratio=1.0):
#     """
#     Applies MCTF to this transformer. Afterward, set r using model.r.

#     If you want to know the source of each token (e.g., for visualization), set trace_source = true.
#     The sources will be available at model._tome_info["source"] afterward.

#     For proportional attention, set prop_attn to True. This is only necessary when evaluating models off
#     the shelf. For trianing and for evaluating MAE models off the self set this to be False.
#     """
#     MCTFBartModel = make_tome_class(model.__class__)
#     print('using', 'tome')

#     model.__class__ = MCTFBartModel
#     model.encoder.__class__ = BartEncoder
#     model.ratio = ratio 
#     # model.compress_method = 'tome' 
#     model._tome_info = {
#         "ratio": model.ratio,
#         "margin":  [],
#         "size": None,
#         "source": None,
#         "trace_source": trace_source,
#         "prop_attn": prop_attn,
#         "class_token": False, 
#         "distill_token": False,
#     }
#     model.init_r()

#     current_layer = 0
#     margin = margin 
#     num_layers = len(model.encoder.layers)

#     if hasattr(model, "dist_token") and model.dist_token is not None:
#         model._tome_info["distill_token"] = True

#     for module in model.encoder.modules():
#         if isinstance(module, BartEncoder):
#             module.__class__ = MCTFBartEncoder
#         if isinstance(module, BartEncoderLayer):
#             module.__class__ = MCTFBartEncoderLayer 
#             module._tome_info = model._tome_info
#             current_layer +=1
#         elif isinstance(module, BartAttention):
#             module.__class__ = MCTFBartAttention
